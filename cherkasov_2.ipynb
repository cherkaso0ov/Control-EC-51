{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загружаем наш дата сет. Мы имеем дата сет из 10 колонок (Age, Sex, ALP, ALT, AST, BIL, CHE, CHOL, CREA, Category) и 615 строк. Есть как числовые, так и качественные переменные. Крайний столбец Category выступает как классификатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем необходимые библиотеки для начала работы (Numpy, Matplotlib.pyplot ,Pandas)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем дата сет в формате сsv, разделитель запятые.\n",
    "df = pd.read_csv('cont.csv', sep=',' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ALP</th>\n",
       "      <th>ALT</th>\n",
       "      <th>AST</th>\n",
       "      <th>BIL</th>\n",
       "      <th>CHE</th>\n",
       "      <th>CHOL</th>\n",
       "      <th>CREA</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>52.5</td>\n",
       "      <td>7.7</td>\n",
       "      <td>22.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>6.93</td>\n",
       "      <td>3.23</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>70.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>11.17</td>\n",
       "      <td>4.80</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>74.7</td>\n",
       "      <td>36.2</td>\n",
       "      <td>52.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>8.84</td>\n",
       "      <td>5.20</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>52.0</td>\n",
       "      <td>30.6</td>\n",
       "      <td>22.6</td>\n",
       "      <td>18.9</td>\n",
       "      <td>7.33</td>\n",
       "      <td>4.74</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>74.1</td>\n",
       "      <td>32.6</td>\n",
       "      <td>24.8</td>\n",
       "      <td>9.6</td>\n",
       "      <td>9.15</td>\n",
       "      <td>4.32</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>62</td>\n",
       "      <td>f</td>\n",
       "      <td>416.6</td>\n",
       "      <td>5.9</td>\n",
       "      <td>110.3</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.57</td>\n",
       "      <td>6.30</td>\n",
       "      <td>55.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>64</td>\n",
       "      <td>f</td>\n",
       "      <td>102.8</td>\n",
       "      <td>2.9</td>\n",
       "      <td>44.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.54</td>\n",
       "      <td>3.02</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>64</td>\n",
       "      <td>f</td>\n",
       "      <td>87.3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>99.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.63</td>\n",
       "      <td>66.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>46</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.56</td>\n",
       "      <td>4.20</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>59</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.07</td>\n",
       "      <td>5.30</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>615 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age Sex    ALP    ALT    AST   BIL    CHE  CHOL   CREA  Category\n",
       "0     32   m   52.5    7.7   22.1   7.5   6.93  3.23  106.0         0\n",
       "1     32   m   70.3   18.0   24.7   3.9  11.17  4.80   74.0         0\n",
       "2     32   m   74.7   36.2   52.6   6.1   8.84  5.20   86.0         0\n",
       "3     32   m   52.0   30.6   22.6  18.9   7.33  4.74   80.0         0\n",
       "4     32   m   74.1   32.6   24.8   9.6   9.15  4.32   76.0         0\n",
       "..   ...  ..    ...    ...    ...   ...    ...   ...    ...       ...\n",
       "610   62   f  416.6    5.9  110.3  50.0   5.57  6.30   55.7         1\n",
       "611   64   f  102.8    2.9   44.4  20.0   1.54  3.02   63.0         1\n",
       "612   64   f   87.3    3.5   99.0  48.0   1.66  3.63   66.7         1\n",
       "613   46   f    NaN   39.0   62.0  20.0   3.56  4.20   52.0         1\n",
       "614   59   f    NaN  100.0   80.0  12.0   9.07  5.30   67.0         1\n",
       "\n",
       "[615 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверяем как загрузился дата сет. Сразу видно присутствие NaN в столбцах\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверяем и заменяем пропуски в числовых столбцах дата сета.\n",
    "def fill_missing_num(x):\n",
    "    num_var = list(x._get_numeric_data().columns)\n",
    "    for col_names in num_var:        \n",
    "        prep_med = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        prep_med.fit(x[num_var])\n",
    "        x[num_var] = prep_med.transform(x[num_var])\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заполняем значения NaN с помощью SimpleImputer.\n",
    "from sklearn.impute import SimpleImputer\n",
    "df = fill_missing_num(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Производим кодирование всех качественных переменных, в нашем случае только столбец Sex.\n",
    "def encoding_char(x):\n",
    "    char_var = list(set(x.columns) - set(x._get_numeric_data().columns))\n",
    "    for col_names in char_var:\n",
    "        f = pd.factorize(x[col_names])\n",
    "        x[col_names] = pd.factorize(x[col_names])[0]\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = encoding_char(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Делим наш дата сет на обучающую и тестирующую выборку в пропорции 20/80.\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Производим шкалирование наших данных без нашей эдогенной переменной Category.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler().fit(X_train)\n",
    "X_train = sc_X.transform(X_train)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.312738\n",
      "         Iterations 8\n",
      "                         Results: Logit\n",
      "=================================================================\n",
      "Model:              Logit            Pseudo R-squared: 0.214     \n",
      "Dependent Variable: y                AIC:              325.7338  \n",
      "Date:               2020-11-23 11:23 BIC:              363.5201  \n",
      "No. Observations:   492              Log-Likelihood:   -153.87   \n",
      "Df Model:           8                LL-Null:          -195.80   \n",
      "Df Residuals:       483              LLR p-value:      8.1273e-15\n",
      "Converged:          1.0000           Scale:            1.0000    \n",
      "No. Iterations:     8.0000                                       \n",
      "--------------------------------------------------------------------\n",
      "       Coef.     Std.Err.       z       P>|z|      [0.025     0.975]\n",
      "--------------------------------------------------------------------\n",
      "x1     0.0023      0.1505     0.0155    0.9877    -0.2927     0.2973\n",
      "x2     0.1901      0.1652     1.1504    0.2500    -0.1338     0.5139\n",
      "x3    -0.0718      0.2285    -0.3145    0.7532    -0.5196     0.3760\n",
      "x4    -1.4307      0.2335    -6.1277    0.0000    -1.8883    -0.9731\n",
      "x5     7.3728      0.6351    11.6094    0.0000     6.1281     8.6175\n",
      "x6     1.1551      0.3660     3.1556    0.0016     0.4376     1.8725\n",
      "x7    -0.2509      0.1840    -1.3632    0.1728    -0.6115     0.1098\n",
      "x8    -0.1494      0.1762    -0.8479    0.3965    -0.4947     0.1959\n",
      "x9     0.0912      0.1883     0.4845    0.6281    -0.2779     0.4604\n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Строим базовую модель и смотрим на полученный отчет. Можно сделать вывод, что значимых перменных только две, это х4 (ALT) и х5 (AST). Будем использовать их для построения наших классификаторов.\n",
    "import statsmodels.api as sm\n",
    "lr = sm.Logit(y_train, X_train).fit()\n",
    "print(lr.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[False False False False  True  True False  True  True]\n",
      "[4 6 2 3 1 1 5 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Automatic Feature Selection\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "estimator=LogisticRegression(random_state=123)\n",
    "selector=RFECV(estimator, step=1)\n",
    "selector=selector.fit(X_train, y_train)\n",
    "print(selector.n_features_)\n",
    "print(selector.support_)\n",
    "print(selector.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 7, 8]\n",
      "№\tIndex\tFeature\n",
      "0\t4\tAST\n",
      "1\t5\tBIL\n",
      "2\t7\tCHOL\n",
      "3\t8\tCREA\n"
     ]
    }
   ],
   "source": [
    "# Selected Features\n",
    "selected_columns=[]\n",
    "for i in range(len(X_train[0])):\n",
    "    if selector.support_[i]==1:\n",
    "        selected_columns.append(i)\n",
    "print(selected_columns)\n",
    "print('№\\tIndex\\tFeature')\n",
    "i=0\n",
    "for column in selected_columns:\n",
    "    print(str(i)+'\\t'+str(column)+'\\t'+str(df.columns[column]))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_select=X_train[:, selected_columns]\n",
    "X_test_select=X_test[:, selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Остлавляем только значимые переменные.\n",
    "X_train = X_train[:,[3,4]]\n",
    "X_test = X_test[:,[3,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем модель логистической регрессии.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state = 13).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.959349593495935"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Делаем прогноз на тестовой выборке. Уровень качества модели очень высок, превышает 95%, то  есть 95% объектов распознано верно.\n",
    "y_pred = lr.predict(X_test)\n",
    "lr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[108   0]\n",
      " [  5  10]]\n"
     ]
    }
   ],
   "source": [
    "# Строим таблицу сопряженности. Можно сделать вывод, что 5 положительных случая ложно определены как отрицательные. \n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
      "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Строим область значений. Они отделяются линейно. Зелёные точки - больные люди, красные точки - здоровые. Люди с повышенным значением AST более склонны к болезни. \n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test, y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, lr.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Logistic Regression (Test set)')\n",
    "plt.xlabel('ALT')\n",
    "plt.ylabel('AST')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Устанавливаем необходимые пакеты Keras.\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Строим нейронную сеть прямой передачи сигнала. На входном слое 8 нейрона (усложнее). Используем функцию relu, которая позволит учесть наличие линейнносоти в исходных данных. На втором слое выбирем 1 нейрон используя сигмоидальную функцию. \n",
    "cnn = Sequential()\n",
    "cnn.add(Dense(units = 8,  activation = 'relu', input_dim = 2))\n",
    "cnn.add(Dense(units = 1,  activation = 'sigmoid'))\n",
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "62/62 [==============================] - 0s 479us/step - loss: 0.7554 - accuracy: 0.2683\n",
      "Epoch 2/100\n",
      "62/62 [==============================] - 0s 466us/step - loss: 0.6316 - accuracy: 0.8374\n",
      "Epoch 3/100\n",
      "62/62 [==============================] - 0s 460us/step - loss: 0.5462 - accuracy: 0.8679\n",
      "Epoch 4/100\n",
      "62/62 [==============================] - 0s 466us/step - loss: 0.4816 - accuracy: 0.8679\n",
      "Epoch 5/100\n",
      "62/62 [==============================] - 0s 466us/step - loss: 0.4291 - accuracy: 0.8740\n",
      "Epoch 6/100\n",
      "62/62 [==============================] - 0s 483us/step - loss: 0.3866 - accuracy: 0.8862\n",
      "Epoch 7/100\n",
      "62/62 [==============================] - 0s 486us/step - loss: 0.3521 - accuracy: 0.8882\n",
      "Epoch 8/100\n",
      "62/62 [==============================] - 0s 451us/step - loss: 0.3243 - accuracy: 0.8923\n",
      "Epoch 9/100\n",
      "62/62 [==============================] - 0s 466us/step - loss: 0.3017 - accuracy: 0.8943\n",
      "Epoch 10/100\n",
      "62/62 [==============================] - 0s 464us/step - loss: 0.2832 - accuracy: 0.8943\n",
      "Epoch 11/100\n",
      "62/62 [==============================] - 0s 482us/step - loss: 0.2678 - accuracy: 0.8943\n",
      "Epoch 12/100\n",
      "62/62 [==============================] - 0s 464us/step - loss: 0.2550 - accuracy: 0.9024\n",
      "Epoch 13/100\n",
      "62/62 [==============================] - 0s 462us/step - loss: 0.2445 - accuracy: 0.9045\n",
      "Epoch 14/100\n",
      "62/62 [==============================] - 0s 450us/step - loss: 0.2356 - accuracy: 0.9045\n",
      "Epoch 15/100\n",
      "62/62 [==============================] - 0s 450us/step - loss: 0.2284 - accuracy: 0.9045\n",
      "Epoch 16/100\n",
      "62/62 [==============================] - 0s 518us/step - loss: 0.2222 - accuracy: 0.9106\n",
      "Epoch 17/100\n",
      "62/62 [==============================] - 0s 451us/step - loss: 0.2166 - accuracy: 0.9167\n",
      "Epoch 18/100\n",
      "62/62 [==============================] - 0s 476us/step - loss: 0.2121 - accuracy: 0.9167\n",
      "Epoch 19/100\n",
      "62/62 [==============================] - 0s 483us/step - loss: 0.2081 - accuracy: 0.9228\n",
      "Epoch 20/100\n",
      "62/62 [==============================] - 0s 466us/step - loss: 0.2044 - accuracy: 0.9268\n",
      "Epoch 21/100\n",
      "62/62 [==============================] - 0s 466us/step - loss: 0.2009 - accuracy: 0.9289\n",
      "Epoch 22/100\n",
      "62/62 [==============================] - 0s 542us/step - loss: 0.1977 - accuracy: 0.9289\n",
      "Epoch 23/100\n",
      "62/62 [==============================] - 0s 467us/step - loss: 0.1949 - accuracy: 0.9309\n",
      "Epoch 24/100\n",
      "62/62 [==============================] - 0s 503us/step - loss: 0.1921 - accuracy: 0.9370\n",
      "Epoch 25/100\n",
      "62/62 [==============================] - 0s 464us/step - loss: 0.1895 - accuracy: 0.9370\n",
      "Epoch 26/100\n",
      "62/62 [==============================] - 0s 467us/step - loss: 0.1871 - accuracy: 0.9390\n",
      "Epoch 27/100\n",
      "62/62 [==============================] - 0s 498us/step - loss: 0.1851 - accuracy: 0.9390\n",
      "Epoch 28/100\n",
      "62/62 [==============================] - 0s 460us/step - loss: 0.1830 - accuracy: 0.9390\n",
      "Epoch 29/100\n",
      "62/62 [==============================] - 0s 502us/step - loss: 0.1811 - accuracy: 0.9411\n",
      "Epoch 30/100\n",
      "62/62 [==============================] - 0s 466us/step - loss: 0.1795 - accuracy: 0.9411\n",
      "Epoch 31/100\n",
      "62/62 [==============================] - 0s 483us/step - loss: 0.1777 - accuracy: 0.9411\n",
      "Epoch 32/100\n",
      "62/62 [==============================] - 0s 477us/step - loss: 0.1763 - accuracy: 0.9370\n",
      "Epoch 33/100\n",
      "62/62 [==============================] - 0s 450us/step - loss: 0.1748 - accuracy: 0.9370\n",
      "Epoch 34/100\n",
      "62/62 [==============================] - 0s 450us/step - loss: 0.1738 - accuracy: 0.9370\n",
      "Epoch 35/100\n",
      "62/62 [==============================] - 0s 515us/step - loss: 0.1723 - accuracy: 0.9390\n",
      "Epoch 36/100\n",
      "62/62 [==============================] - 0s 483us/step - loss: 0.1712 - accuracy: 0.9390\n",
      "Epoch 37/100\n",
      "62/62 [==============================] - 0s 483us/step - loss: 0.1700 - accuracy: 0.9390\n",
      "Epoch 38/100\n",
      "62/62 [==============================] - 0s 483us/step - loss: 0.1690 - accuracy: 0.9390\n",
      "Epoch 39/100\n",
      "62/62 [==============================] - 0s 467us/step - loss: 0.1684 - accuracy: 0.9390\n",
      "Epoch 40/100\n",
      "62/62 [==============================] - 0s 595us/step - loss: 0.1672 - accuracy: 0.9411\n",
      "Epoch 41/100\n",
      "62/62 [==============================] - 0s 488us/step - loss: 0.1658 - accuracy: 0.9411\n",
      "Epoch 42/100\n",
      "62/62 [==============================] - 0s 531us/step - loss: 0.1645 - accuracy: 0.9411\n",
      "Epoch 43/100\n",
      "62/62 [==============================] - 0s 517us/step - loss: 0.1631 - accuracy: 0.9411\n",
      "Epoch 44/100\n",
      "62/62 [==============================] - 0s 515us/step - loss: 0.1616 - accuracy: 0.9411\n",
      "Epoch 45/100\n",
      "62/62 [==============================] - 0s 531us/step - loss: 0.1592 - accuracy: 0.9431\n",
      "Epoch 46/100\n",
      "62/62 [==============================] - 0s 515us/step - loss: 0.1569 - accuracy: 0.9431\n",
      "Epoch 47/100\n",
      "62/62 [==============================] - 0s 467us/step - loss: 0.1555 - accuracy: 0.9431\n",
      "Epoch 48/100\n",
      "62/62 [==============================] - 0s 515us/step - loss: 0.1539 - accuracy: 0.9431\n",
      "Epoch 49/100\n",
      "62/62 [==============================] - 0s 475us/step - loss: 0.1524 - accuracy: 0.9431\n",
      "Epoch 50/100\n",
      "62/62 [==============================] - 0s 472us/step - loss: 0.1509 - accuracy: 0.9431\n",
      "Epoch 51/100\n",
      "62/62 [==============================] - 0s 435us/step - loss: 0.1496 - accuracy: 0.9431\n",
      "Epoch 52/100\n",
      "62/62 [==============================] - 0s 531us/step - loss: 0.1483 - accuracy: 0.9411\n",
      "Epoch 53/100\n",
      "62/62 [==============================] - 0s 515us/step - loss: 0.1471 - accuracy: 0.9431\n",
      "Epoch 54/100\n",
      "62/62 [==============================] - 0s 450us/step - loss: 0.1460 - accuracy: 0.9431\n",
      "Epoch 55/100\n",
      "62/62 [==============================] - 0s 466us/step - loss: 0.1450 - accuracy: 0.9431\n",
      "Epoch 56/100\n",
      "62/62 [==============================] - 0s 450us/step - loss: 0.1439 - accuracy: 0.9431\n",
      "Epoch 57/100\n",
      "62/62 [==============================] - 0s 442us/step - loss: 0.1430 - accuracy: 0.9431\n",
      "Epoch 58/100\n",
      "62/62 [==============================] - 0s 434us/step - loss: 0.1420 - accuracy: 0.9431\n",
      "Epoch 59/100\n",
      "62/62 [==============================] - 0s 435us/step - loss: 0.1410 - accuracy: 0.9431\n",
      "Epoch 60/100\n",
      "62/62 [==============================] - 0s 466us/step - loss: 0.1402 - accuracy: 0.9431\n",
      "Epoch 61/100\n",
      "62/62 [==============================] - 0s 450us/step - loss: 0.1394 - accuracy: 0.9431\n",
      "Epoch 62/100\n",
      "62/62 [==============================] - 0s 450us/step - loss: 0.1388 - accuracy: 0.9431\n",
      "Epoch 63/100\n",
      "62/62 [==============================] - 0s 450us/step - loss: 0.1384 - accuracy: 0.9431\n",
      "Epoch 64/100\n",
      "62/62 [==============================] - 0s 442us/step - loss: 0.1376 - accuracy: 0.9431\n",
      "Epoch 65/100\n",
      "62/62 [==============================] - 0s 453us/step - loss: 0.1371 - accuracy: 0.9451\n",
      "Epoch 66/100\n",
      "62/62 [==============================] - 0s 440us/step - loss: 0.1363 - accuracy: 0.9451\n",
      "Epoch 67/100\n",
      "62/62 [==============================] - 0s 464us/step - loss: 0.1364 - accuracy: 0.9451\n",
      "Epoch 68/100\n",
      "62/62 [==============================] - 0s 439us/step - loss: 0.1352 - accuracy: 0.9472\n",
      "Epoch 69/100\n",
      "62/62 [==============================] - 0s 450us/step - loss: 0.1347 - accuracy: 0.9472\n",
      "Epoch 70/100\n",
      "62/62 [==============================] - 0s 450us/step - loss: 0.1342 - accuracy: 0.9492\n",
      "Epoch 71/100\n",
      "62/62 [==============================] - 0s 450us/step - loss: 0.1337 - accuracy: 0.9492\n",
      "Epoch 72/100\n",
      "62/62 [==============================] - 0s 439us/step - loss: 0.1332 - accuracy: 0.9492\n",
      "Epoch 73/100\n",
      "62/62 [==============================] - 0s 434us/step - loss: 0.1330 - accuracy: 0.9512\n",
      "Epoch 74/100\n",
      "62/62 [==============================] - 0s 463us/step - loss: 0.1322 - accuracy: 0.9492\n",
      "Epoch 75/100\n",
      "62/62 [==============================] - 0s 466us/step - loss: 0.1320 - accuracy: 0.9512\n",
      "Epoch 76/100\n",
      "62/62 [==============================] - 0s 480us/step - loss: 0.1316 - accuracy: 0.9512\n",
      "Epoch 77/100\n",
      "62/62 [==============================] - 0s 499us/step - loss: 0.1311 - accuracy: 0.9512\n",
      "Epoch 78/100\n",
      "62/62 [==============================] - 0s 473us/step - loss: 0.1308 - accuracy: 0.9512\n",
      "Epoch 79/100\n",
      "62/62 [==============================] - 0s 498us/step - loss: 0.1305 - accuracy: 0.9512\n",
      "Epoch 80/100\n",
      "62/62 [==============================] - 0s 466us/step - loss: 0.1303 - accuracy: 0.9533\n",
      "Epoch 81/100\n",
      "62/62 [==============================] - 0s 489us/step - loss: 0.1298 - accuracy: 0.9533\n",
      "Epoch 82/100\n",
      "62/62 [==============================] - 0s 479us/step - loss: 0.1295 - accuracy: 0.9533\n",
      "Epoch 83/100\n",
      "62/62 [==============================] - 0s 450us/step - loss: 0.1291 - accuracy: 0.9533\n",
      "Epoch 84/100\n",
      "62/62 [==============================] - 0s 468us/step - loss: 0.1288 - accuracy: 0.9553\n",
      "Epoch 85/100\n",
      "62/62 [==============================] - 0s 450us/step - loss: 0.1285 - accuracy: 0.9553\n",
      "Epoch 86/100\n",
      "62/62 [==============================] - 0s 467us/step - loss: 0.1284 - accuracy: 0.9553\n",
      "Epoch 87/100\n",
      "62/62 [==============================] - 0s 483us/step - loss: 0.1279 - accuracy: 0.9553\n",
      "Epoch 88/100\n",
      "62/62 [==============================] - 0s 499us/step - loss: 0.1277 - accuracy: 0.9553\n",
      "Epoch 89/100\n",
      "62/62 [==============================] - 0s 527us/step - loss: 0.1275 - accuracy: 0.9553\n",
      "Epoch 90/100\n",
      "62/62 [==============================] - 0s 499us/step - loss: 0.1272 - accuracy: 0.9553\n",
      "Epoch 91/100\n",
      "62/62 [==============================] - 0s 499us/step - loss: 0.1269 - accuracy: 0.9553\n",
      "Epoch 92/100\n",
      "62/62 [==============================] - 0s 499us/step - loss: 0.1266 - accuracy: 0.9553\n",
      "Epoch 93/100\n",
      "62/62 [==============================] - 0s 498us/step - loss: 0.1266 - accuracy: 0.9553\n",
      "Epoch 94/100\n",
      "62/62 [==============================] - 0s 644us/step - loss: 0.1264 - accuracy: 0.9553\n",
      "Epoch 95/100\n",
      "62/62 [==============================] - 0s 483us/step - loss: 0.1262 - accuracy: 0.9553\n",
      "Epoch 96/100\n",
      "62/62 [==============================] - 0s 483us/step - loss: 0.1262 - accuracy: 0.9573\n",
      "Epoch 97/100\n",
      "62/62 [==============================] - 0s 463us/step - loss: 0.1257 - accuracy: 0.9553\n",
      "Epoch 98/100\n",
      "62/62 [==============================] - 0s 531us/step - loss: 0.1255 - accuracy: 0.9553\n",
      "Epoch 99/100\n",
      "62/62 [==============================] - 0s 483us/step - loss: 0.1254 - accuracy: 0.9553\n",
      "Epoch 100/100\n",
      "62/62 [==============================] - 0s 483us/step - loss: 0.1251 - accuracy: 0.9553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19f0b0ffa90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучаем нейронную сеть в течении 100 эпох.\n",
    "cnn.fit(X_train, y_train, epochs = 100, batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Строим прогноз на тестовой выборке.\n",
    "y_pred = cnn.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[108   0]\n",
      " [  3  12]]\n"
     ]
    }
   ],
   "source": [
    "# Строим таблицу сопряженности. Можно сделать вывод, что 3 положительных случая ложно определены как отрицательные, что уже лучше, чем модель построенная ранее.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
      "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Строим область значений. Они отделяются менее линейно в сравнении с предыдущей моделью. Однако после усложения модели после увеличения входных нейронов с 2 до 8, качество модели улучшилось. \n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test, y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, cnn.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('NN (Test set)')\n",
    "plt.xlabel('ALT')\n",
    "plt.ylabel('AST')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
